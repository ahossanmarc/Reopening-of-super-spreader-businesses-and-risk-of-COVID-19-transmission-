{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>MIT Covid-19 Datathon</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Tool Imports\n",
    "# Use pandas for dataframe tools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These definitions are run time environment specific\n",
    "data_path = '/home/michael/Documents/Data/safegraph/v1/main-file/'\n",
    "backfill_path = '/home/michael/Documents/Data/safegraph/patterns_backfill/2020/05/07/12/2019/'\n",
    "sub_dirs = ['01/', '02/', '03/', '04/', '05/', '06/', '07/', '08/', '09/', '10/', '11/', '12/']\n",
    "#sub_dirs = ['01/', '02/', '03/', '04/']\n",
    "backfill_files = ['patterns-part1.csv', 'patterns-part2.csv', 'patterns-part3.csv', 'patterns-part4.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the files we want to process\n",
    "visit_data_files = ['2020-04-26-weekly-patterns.csv']\n",
    "\n",
    "core_poi_data_files = ['core_poi-part1.csv', 'core_poi-part2.csv','core_poi-part3.csv',\\\n",
    "                      'core_poi-part4.csv', 'core_poi-part5.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our operating parameters\n",
    "focus_states = ['RI', 'CT']\n",
    "\n",
    "# Define NCIS Codes to save\n",
    "\n",
    "# Pattern file columns to keep\n",
    "pattern_keep_columns = ['safegraph_place_id', 'location_name', 'region', 'date_range_start', 'date_range_end', \\\n",
    "                        'postal_code',  'raw_visit_counts', 'median_dwell', 'bucketed_dwell_times']\n",
    "\n",
    "core_poi_keep_columns = ['safegraph_place_id', 'latitude', 'longitude', 'naics_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will split the bucketed dwell field and calculate an estimated mean\n",
    "def calc_dwells(dwell_col) :\n",
    "    total_dwell_list = []\n",
    "    dwell_buckets = {}\n",
    "    bucket_multipliers = {'<5' : 2.5, '5-20' : 12.5, '21-60' : 40.5, '61-240' : 150.5, '>240' : 240}\n",
    "    \n",
    "    # run through each row in the series\n",
    "    \n",
    "    for row in dwell_col :\n",
    "        # ignore the starting and ending brackets\n",
    "        entries = row[1:-1].split(',')\n",
    "\n",
    "        # for each bucket, split the key from the value\n",
    "        for entry in entries:\n",
    "            bucket, count = entry.split(':')\n",
    "            dwell_buckets[bucket.replace('\\\"', '')] = int(count)\n",
    "\n",
    "\n",
    "            total_dwell = 0\n",
    "            \n",
    "            # Now use the dictionary to calculate the total dwell time\n",
    "            for bucket in dwell_buckets :\n",
    "                total_dwell += dwell_buckets[bucket] * bucket_multipliers[bucket]\n",
    "\n",
    "        total_dwell_list.append(total_dwell)\n",
    "    \n",
    "    # return the list of total dwell times\n",
    "    return(total_dwell_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = pd.read_csv('placeCountyCBG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This series of steps creates a dataframe with all the core poi entries\n",
    "\n",
    "# initialize the data frame\n",
    "big_core_df = pd.read_csv(data_path+core_poi_data_files[0])\n",
    "\n",
    "# only keep the columns we'll use in our analysis\n",
    "big_core_df = big_core_df[core_poi_keep_columns]\n",
    "\n",
    "# Run through all the other files in the list and append\n",
    "for file_name in core_poi_data_files[1:]:\n",
    "    try:\n",
    "        # read the next data file and concat on the end\n",
    "        df2 = pd.read_csv(data_path+file_name)\n",
    "        df2 = df2[core_poi_keep_columns]\n",
    "        big_core_df = pd.concat([big_core_df, df2])\n",
    "        \n",
    "        # Force the deletion of df2 to save memory\n",
    "        del df2\n",
    "    except OSError :\n",
    "        print(\"Unable to open file {}\".format(data_path+file_name))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backfill-patterns1-01-2019-RI.csv\n",
      "backfill-patterns1-01-2019-CT.csv\n",
      "backfill-patterns2-01-2019-RI.csv\n",
      "backfill-patterns2-01-2019-CT.csv\n",
      "backfill-patterns3-01-2019-RI.csv\n",
      "backfill-patterns3-01-2019-CT.csv\n",
      "backfill-patterns4-01-2019-RI.csv\n",
      "backfill-patterns4-01-2019-CT.csv\n",
      "backfill-patterns1-02-2019-RI.csv\n",
      "backfill-patterns1-02-2019-CT.csv\n",
      "backfill-patterns2-02-2019-RI.csv\n",
      "backfill-patterns2-02-2019-CT.csv\n",
      "backfill-patterns3-02-2019-RI.csv\n",
      "backfill-patterns3-02-2019-CT.csv\n",
      "backfill-patterns4-02-2019-RI.csv\n",
      "backfill-patterns4-02-2019-CT.csv\n",
      "backfill-patterns1-03-2019-RI.csv\n",
      "backfill-patterns1-03-2019-CT.csv\n",
      "backfill-patterns2-03-2019-RI.csv\n",
      "backfill-patterns2-03-2019-CT.csv\n",
      "backfill-patterns3-03-2019-RI.csv\n",
      "backfill-patterns3-03-2019-CT.csv\n",
      "backfill-patterns4-03-2019-RI.csv\n",
      "backfill-patterns4-03-2019-CT.csv\n",
      "backfill-patterns1-04-2019-RI.csv\n",
      "backfill-patterns1-04-2019-CT.csv\n",
      "backfill-patterns2-04-2019-RI.csv\n",
      "backfill-patterns2-04-2019-CT.csv\n",
      "backfill-patterns3-04-2019-RI.csv\n",
      "backfill-patterns3-04-2019-CT.csv\n",
      "backfill-patterns4-04-2019-RI.csv\n",
      "backfill-patterns4-04-2019-CT.csv\n",
      "backfill-patterns1-05-2019-RI.csv\n",
      "backfill-patterns1-05-2019-CT.csv\n",
      "backfill-patterns2-05-2019-RI.csv\n",
      "backfill-patterns2-05-2019-CT.csv\n",
      "backfill-patterns3-05-2019-RI.csv\n",
      "backfill-patterns3-05-2019-CT.csv\n",
      "backfill-patterns4-05-2019-RI.csv\n",
      "backfill-patterns4-05-2019-CT.csv\n",
      "backfill-patterns1-06-2019-RI.csv\n",
      "backfill-patterns1-06-2019-CT.csv\n",
      "backfill-patterns2-06-2019-RI.csv\n",
      "backfill-patterns2-06-2019-CT.csv\n",
      "backfill-patterns3-06-2019-RI.csv\n",
      "backfill-patterns3-06-2019-CT.csv\n",
      "backfill-patterns4-06-2019-RI.csv\n",
      "backfill-patterns4-06-2019-CT.csv\n",
      "backfill-patterns1-07-2019-RI.csv\n",
      "backfill-patterns1-07-2019-CT.csv\n",
      "backfill-patterns2-07-2019-RI.csv\n",
      "backfill-patterns2-07-2019-CT.csv\n",
      "backfill-patterns3-07-2019-RI.csv\n",
      "backfill-patterns3-07-2019-CT.csv\n",
      "backfill-patterns4-07-2019-RI.csv\n",
      "backfill-patterns4-07-2019-CT.csv\n",
      "backfill-patterns1-08-2019-RI.csv\n",
      "backfill-patterns1-08-2019-CT.csv\n",
      "backfill-patterns2-08-2019-RI.csv\n",
      "backfill-patterns2-08-2019-CT.csv\n",
      "backfill-patterns3-08-2019-RI.csv\n",
      "backfill-patterns3-08-2019-CT.csv\n",
      "backfill-patterns4-08-2019-RI.csv\n",
      "backfill-patterns4-08-2019-CT.csv\n",
      "backfill-patterns1-09-2019-RI.csv\n",
      "backfill-patterns1-09-2019-CT.csv\n",
      "backfill-patterns2-09-2019-RI.csv\n",
      "backfill-patterns2-09-2019-CT.csv\n",
      "backfill-patterns3-09-2019-RI.csv\n",
      "backfill-patterns3-09-2019-CT.csv\n",
      "backfill-patterns4-09-2019-RI.csv\n",
      "backfill-patterns4-09-2019-CT.csv\n",
      "backfill-patterns1-10-2019-RI.csv\n",
      "backfill-patterns1-10-2019-CT.csv\n",
      "backfill-patterns2-10-2019-RI.csv\n",
      "backfill-patterns2-10-2019-CT.csv\n",
      "backfill-patterns3-10-2019-RI.csv\n",
      "backfill-patterns3-10-2019-CT.csv\n",
      "backfill-patterns4-10-2019-RI.csv\n",
      "backfill-patterns4-10-2019-CT.csv\n",
      "backfill-patterns1-11-2019-RI.csv\n",
      "backfill-patterns1-11-2019-CT.csv\n",
      "backfill-patterns2-11-2019-RI.csv\n",
      "backfill-patterns2-11-2019-CT.csv\n",
      "backfill-patterns3-11-2019-RI.csv\n",
      "backfill-patterns3-11-2019-CT.csv\n",
      "backfill-patterns4-11-2019-RI.csv\n",
      "backfill-patterns4-11-2019-CT.csv\n",
      "backfill-patterns1-12-2019-RI.csv\n",
      "backfill-patterns1-12-2019-CT.csv\n",
      "backfill-patterns2-12-2019-RI.csv\n",
      "backfill-patterns2-12-2019-CT.csv\n",
      "backfill-patterns3-12-2019-RI.csv\n",
      "backfill-patterns3-12-2019-CT.csv\n",
      "backfill-patterns4-12-2019-RI.csv\n",
      "backfill-patterns4-12-2019-CT.csv\n"
     ]
    }
   ],
   "source": [
    "# The safegraph data is stored in a subdirectory for each month\n",
    "for sd in sub_dirs:\n",
    "    idx = 0;\n",
    "    \n",
    "    # Each month is divided into four files with standard names\n",
    "    for file_name in backfill_files:\n",
    "        idx += 1;\n",
    "        \n",
    "        # Read the first datafile into a dataframe\n",
    "        df = pd.read_csv(backfill_path+sd+file_name)\n",
    "        \n",
    "        # only keep the columns of interest\n",
    "        df = df[pattern_keep_columns]\n",
    "        \n",
    "        # Calculate the total dwell from the buckets\n",
    "        df['est_total_dwell'] = calc_dwells(df['bucketed_dwell_times'])\n",
    "        \n",
    "        # Merge the file with the core poi info and the location information\n",
    "        big_df = df.merge(big_core_df, how='left')\n",
    "        big_df = big_df.merge(loc_df, how='left')\n",
    "\n",
    "        # Filter the dataframe by state and save a state specific data file\n",
    "        for state in focus_states :\n",
    "            new_big_df = big_df[big_df['region'] == state]\n",
    "            new_file_name = 'backfill-patterns'+str(idx)+'-'+sd[:-1]+'-2019-'+state+'.csv'\n",
    "            new_big_df.to_csv(new_file_name)\n",
    "            print(new_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
